{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from __future__ import division\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "board = \"| {0} | {1} | {2} |\\n-------------\\n| {3} | {4} | {5} |\\n-------------\\n| {6} | {7} | {8} |\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(output_dim=60, input_dim=9))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=25,))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(output_dim=9,))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tie(state):\n",
    "    if 0 not in state and not win(state,1) and not win(state,2): \n",
    "        return True\n",
    "    else: return False\n",
    "    \n",
    "def win(state, token):\n",
    "    if token == state[0] == state[1] == state[2]: return True\n",
    "    if token == state[3] == state[4] == state[5]: return True\n",
    "    if token == state[6] == state[7] == state[8]: return True\n",
    "    if token == state[0] == state[3] == state[6]: return True\n",
    "    if token == state[1] == state[4] == state[7]: return True\n",
    "    if token == state[2] == state[5] == state[8]: return True\n",
    "    if token == state[0] == state[4] == state[8]: return True\n",
    "    if token == state[6] == state[4] == state[2]: return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "class QTable(object):   #This class is a Q-table learning algorithm\n",
    "    def __init__(self):\n",
    "        self.Q = {} # Dictionary will be formatted as {((state), action): q-value}. \n",
    "\n",
    "    def legalRand(self, state): # Returns a random legal move in the state given\n",
    "        possible = []\n",
    "        for i in range(9):\n",
    "            if state[i] == 0:\n",
    "                possible.append(i)\n",
    "        move_index = random.choice(possible)\n",
    "        return move_index\n",
    "    \n",
    "    def legal(self, state): # Returns a list of all legal moves in the state given\n",
    "        legal = []\n",
    "        for i in range(9):\n",
    "            if state[i] == 0:\n",
    "                legal.append(i)\n",
    "        return legal\n",
    "    \n",
    "    def epsilon_greedy(self, epsilon, state): # Makes a random move with probability epsilon\n",
    "        stateA = copy.copy(state)             # Makes the best learned move with probability 1-epsilon\n",
    "        if random.random() > epsilon:\n",
    "            move = self.best_move(state)\n",
    "        else:\n",
    "            move = self.legalRand(state)\n",
    "        stateA[move] = 1\n",
    "        return move, stateA # Returns the move and the new state after having made the move\n",
    "    \n",
    "    def learn(self, games = 10, lrate = .1, discfac = 1, epsilon = .1):\n",
    "        for i in range(games):\n",
    "            state = [0,0,0,0,0,0,0,0,0] \n",
    "            while True:\n",
    "                move, stateA = self.epsilon_greedy(epsilon, state)\n",
    "                if win(stateA, 1): # The agent is rewarded if it wins\n",
    "                    state = tuple(state)\n",
    "                    if (state,move) in self.Q: self.Q[(state,move)] += lrate*(100+discfac*(100)-self.Q[(state,move)]) \n",
    "                    if (state,move) not in self.Q: self.Q[(state,move)] = 0 \n",
    "                    break\n",
    "                if 0 not in stateA: # The agent is rewarded lightly if game ends in a tie\n",
    "                    state = tuple(state)\n",
    "                    if (state,move) in self.Q: self.Q[(state,move)] += lrate*(0+discfac*(100)-self.Q[(state,move)])\n",
    "                    if (state,move) not in self.Q: self.Q[(state,move)] = 0 \n",
    "                    break\n",
    "                else:\n",
    "                    randmove = self.legalRand(stateA)\n",
    "                    stateA[randmove] = 2\n",
    "                    if win(stateA, 2): # The agent is punished if the opponent wins\n",
    "                        state = tuple(state)\n",
    "                        if (state,move) in self.Q: self.Q[(state,move)] += lrate*(0+discfac*(-100)-self.Q[(state,move)]) \n",
    "                        if (state,move) not in self.Q: self.Q[(state,move)] = 0 \n",
    "                        break\n",
    "                    else: # Otherwise, updates to Q values are made normally\n",
    "                        state = tuple(state)\n",
    "                        if (state,move) in self.Q: self.Q[(state,move)] += lrate*(0+discfac*(max(self.nextQs(stateA)))-self.Q[(state,move)])\n",
    "                        if (state,move) not in self.Q: self.Q[(state,move)] = 0\n",
    "                state = stateA\n",
    "        print(\"Done.\")\n",
    "        \n",
    "    def learnSecond(self, games, lrate = .1, discfac = 0.9, epsilon = 0.1):\n",
    "        for i in range(games):\n",
    "            state = [0,0,0,0,0,0,0,0,0]\n",
    "            while True:\n",
    "                randmove = self.legalRand(state)\n",
    "                state[randmove] = 2\n",
    "                #print('state')\n",
    "                #print(state[0], state[1], state[2])\n",
    "                #print(state[3], state[4], state[5])\n",
    "                #print(state[6], state[7], state[8], '\\n')\n",
    "                if win(state, 2):\n",
    "                    state = tuple(state_train)\n",
    "                    #print(\"loss. state\", state, \"move\", move)\n",
    "                    if (state,move) in self.Q: self.Q[(state,move)] += lrate*(0+discfac*(-100)-self.Q[(state,move)]) \n",
    "                    if (state,move) not in self.Q: self.Q[(state,move)] = 0\n",
    "                    break\n",
    "                if 0 not in state: # The agent is rewarded lightly if game ends in a tie\n",
    "                    state = tuple(state_train)\n",
    "                    #print(\"tie. state\", state, \"move\", move)\n",
    "                    if (state,move) in self.Q: self.Q[(state,move)] += lrate*(0+discfac*(10)-self.Q[(state,move)])\n",
    "                    if (state,move) not in self.Q: self.Q[(state,move)] = 0 \n",
    "                    break\n",
    "                else:\n",
    "                    state_train = state\n",
    "                    move, stateA = self.epsilon_greedy(epsilon, state)\n",
    "                    #print('stateA')\n",
    "                    #print(stateA[0], stateA[1], stateA[2])\n",
    "                    #print(stateA[3], stateA[4], stateA[5])\n",
    "                    #print(stateA[6], stateA[7], stateA[8], '\\n')\n",
    "                    if win(stateA, 1): # The agent is rewarded if it wins\n",
    "                        state = tuple(state_train)\n",
    "                        ##print(\"win. state\", state, 'move', move)\n",
    "                        if (state,move) in self.Q: self.Q[(state,move)] += lrate*(100+discfac*(100)-self.Q[(state,move)]) \n",
    "                        if (state,move) not in self.Q: self.Q[(state,move)] = 0 \n",
    "                        break\n",
    "                    else: # Otherwise, updates to Q values are made normally\n",
    "                        state = tuple(state)\n",
    "                        #print(\"nothing. state\", state, \"move\", move)\n",
    "                        if (state,move) in self.Q: self.Q[(state,move)] += lrate*(0+discfac*(max(self.nextQs(stateA)))-self.Q[(state,move)])\n",
    "                        if (state,move) not in self.Q: self.Q[(state,move)] = 0\n",
    "                state = stateA\n",
    "    print(\"Done.\")\n",
    "            \n",
    "    def nextQs(self, state): # Returns a list of Q values associated with each possible move in the state given\n",
    "        possible_moves, q = self.legal(state), []\n",
    "        state = tuple(state)\n",
    "        for i in possible_moves:\n",
    "            if (state,i) in self.Q: q.append(self.Q[(state,i)])\n",
    "            else: q.append(0)\n",
    "        return q\n",
    "    \n",
    "    def best_move(self, state):\n",
    "        possible_moves, q = self.legal(state), self.nextQs(state)\n",
    "        count = q.count(max(q))\n",
    "        if count > 1: # If there is more than one best move, randomly choose one\n",
    "            best_choices = [k for k in range(len(possible_moves)) if q[k] == max(q)]\n",
    "            move_index = random.choice(best_choices)\n",
    "        else: move_index = q.index(max(q)) # Otherwise, choose the best option\n",
    "        return possible_moves[move_index]\n",
    "        \n",
    "    def play(self, state):\n",
    "        move = self.best_move(state)\n",
    "        print(\"Computer's move: {0}\".format(move+1))\n",
    "        return move\n",
    "    \n",
    "    def getType(self):\n",
    "        return \"QTable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  30.09896993637085\n"
     ]
    }
   ],
   "source": [
    "Q_learner = QTable()\n",
    "start = time.time()\n",
    "Q_learner.learnSecond(games = 400000, lrate = .2, discfac = 1, epsilon = .1)\n",
    "end = time.time()\n",
    "print(\"Time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:    #This class is a neural network Q-learning algorithm\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "\n",
    "    def legalRand(self, state): # Returns a random legal move in the state given\n",
    "        possible = []\n",
    "        for i in range(9):\n",
    "            if state[i] == 0:\n",
    "                possible.append(i)\n",
    "        move_index = random.choice(possible)\n",
    "        return move_index\n",
    "    \n",
    "    def legal(self, state): # Returns a list of all legal moves in the state given\n",
    "        legal = []\n",
    "        for i in range(9):\n",
    "            if state[i] == 0:\n",
    "                legal.append(i)\n",
    "        return legal\n",
    "    \n",
    "    # Returns max legal q value\n",
    "    def legalQ(self, Qvals, state):\n",
    "        q = []\n",
    "        legal_moves = self.legal(state)\n",
    "        for i in legal_moves: q.append(Qvals[0][i])\n",
    "        return max(q)\n",
    "    \n",
    "    # Makes a random move with probability epsilon\n",
    "    # Makes the best learned move with probability 1-epsilon\n",
    "    def epsilon_greedy(self, epsilon, state):\n",
    "        stateA = copy.copy(state)         \n",
    "        if random.random() > epsilon:\n",
    "            move = self.best_move(state)\n",
    "        else:\n",
    "            move = self.legalRand(state)\n",
    "        stateA[move] = 1\n",
    "        return move, stateA # Returns the move and the new state after having made the move\n",
    "    \n",
    "    def learn(self, games, discfac, epsilon):\n",
    "        games_won = 0\n",
    "        counter = 0\n",
    "        \n",
    "        for i in range(games):\n",
    "            # Each game starts with an empty board\n",
    "            state = np.array([0,0,0,0,0,0,0,0,0])\n",
    "            \n",
    "            while True:\n",
    "                # Get the NN's output for the current state\n",
    "                qvals = model.predict(state.reshape(1,9), verbose=0)\n",
    "                \n",
    "                # Make the best move (highest Q value) with probability 1-epsilon\n",
    "                # and get the new state after move is made\n",
    "                move, new_state = self.epsilon_greedy(epsilon, state)\n",
    "                \n",
    "                # If the NeuralNetwork wins after this move, or causes a tie, break out of the loop\n",
    "                # to train NN\n",
    "                if win(new_state, 1):\n",
    "                    games_won += 1\n",
    "                    break\n",
    "                    \n",
    "                if tie(new_state): break\n",
    "                \n",
    "                # If we're at this point, the NeuralNetwork didn't win and didn't tie, so\n",
    "                # the opponent makes its move\n",
    "                #if (random.random() >= .5): opp_move = self.legalRand(new_state)\n",
    "                opp_move = self.legalRand(new_state)\n",
    "                \n",
    "                # Put a 2 in the state where the opponent made their move\n",
    "                new_state[opp_move] = 2\n",
    "                # Then if the opponent wins, break to train NN\n",
    "                if win(new_state, 2): break\n",
    "                \n",
    "                # By this point, we know neither the NeuralNetwork or opponent has won,\n",
    "                # so we train NN \n",
    "                else:\n",
    "                    # Reward: 0 since game isn't over\n",
    "                    reward = self.getReward(new_state)\n",
    "                    # Get q values on the state now with the new moves made\n",
    "                    newQ = model.predict(new_state.reshape(1,9), verbose=0)\n",
    "                    # Get max legal q value\n",
    "                    maxQ = self.legalQ(newQ, new_state)\n",
    "                    \n",
    "                    # vector y = qvals, differing only in the spot where the NeuralNetwork\n",
    "                    # chose to move. There, y holds discfac * maxQ. The purpose of this\n",
    "                    # is to train each move based on the best option the NeuralNetwork has in \n",
    "                    # the next state. In other words, moves get judged based on future\n",
    "                    # consequences \n",
    "                    update = reward + (discfac * maxQ)\n",
    "                    y = np.zeros((1,9))\n",
    "                    y[:] = qvals[:] \n",
    "                    y[0][move] = update\n",
    "                    #print(\"Move: %r\" % move)\n",
    "                    #print(\"Update: %r\" % update)\n",
    "                    #print(y)\n",
    "                    # 10 epochs to make sure the behavior is enforced \n",
    "                    self.model.fit(qvals, y, nb_epoch=1, verbose=0)\n",
    "                    # set the current state to the new state\n",
    "                    state = new_state\n",
    "            \n",
    "            # Rewards: win = 10, loss = -10, tie = -10\n",
    "            reward = self.getReward(new_state)\n",
    "            # When the game is over, we train soley based on whether the NeuralNetwork won, lost, or tie\n",
    "            y = np.zeros((1,9))\n",
    "            y[:] = qvals[:]\n",
    "            y[0][move] = reward\n",
    "            #print(\"Move: %r\" % move)\n",
    "            #print(\"Reward %s\" % reward)\n",
    "            #print(y)\n",
    "            model.fit(qvals, y, nb_epoch=1, verbose=0)\n",
    "            \n",
    "            # As training progresses, we make it more likely that NeuralNetwork chooses the best moves\n",
    "            if epsilon > .1: epsilon -= 1/games\n",
    "            #if (reward == 10): games_won += 1\n",
    "            counter += 1\n",
    "            if counter % 10000 == 0: \n",
    "                print (\"Game {0} of {1} complete.\".format(counter, games))\n",
    "                print(\"Epsilon: \", epsilon)\n",
    "        print (\"Done. Won {0} of {1} games.\".format(games_won, games))\n",
    "    \n",
    "    def learnWithQ(self, games, discfac, epsilon, Qplayer):\n",
    "        counter = 0\n",
    "        games_won = 0\n",
    "        for i in range(games):\n",
    "            # Each game starts with an empty board\n",
    "            state = np.array([0,0,0,0,0,0,0,0,0])\n",
    "            j= 0\n",
    "            while True:\n",
    "                reward = 0\n",
    "                # Get the NN's output for the current state\n",
    "                qvals = model.predict(state.reshape(1,9), verbose=0)\n",
    "                \n",
    "                # Make the best move (highest Q value) with probability 1-epsilon\n",
    "                # and get the new state after move is made\n",
    "                move, new_state = self.epsilon_greedy(epsilon, state)\n",
    "                #print(new_state[0], new_state[1], new_state[2])\n",
    "                #print(new_state[3], new_state[4], new_state[5])\n",
    "                #print(new_state[6], new_state[7], new_state[8], '\\n')\n",
    "\n",
    "                # If the NeuralNetwork wins after this move, or causes a tie, break out of the loop\n",
    "                # to train NN\n",
    "                if win(new_state, 1):\n",
    "                    #print(\"win\", j)\n",
    "                    reward = 10\n",
    "                    games_won += 1\n",
    "                    break\n",
    "                    \n",
    "                if tie(new_state):\n",
    "                    reward = 10\n",
    "                    #print(\"tie\")\n",
    "                    #games_won +=1\n",
    "                    break\n",
    "                \n",
    "                # If we're at this point, the NeuralNetwork didn't win and didn't tie, so\n",
    "                # the opponent makes its move\n",
    "                if (random.random() >= 0.6):\n",
    "                    opp_move = Qplayer.best_move(new_state)\n",
    "                else:\n",
    "                    opp_move = self.legalRand(new_state)\n",
    "                \n",
    "                # Put a 2 in the state where the opponent made their move\n",
    "                new_state[opp_move] = 2\n",
    "                #print(new_state[0], new_state[1], new_state[2])\n",
    "                #print(new_state[3], new_state[4], new_state[5])\n",
    "                #print(new_state[6], new_state[7], new_state[8], '\\n')\n",
    "                \n",
    "                # Then if the opponent wins, break to train NN\n",
    "                if win(new_state, 2): \n",
    "                    #print(\"loss\")\n",
    "                    reward = -10\n",
    "                    break\n",
    "                \n",
    "                # By this point, we know neither the NeuralNetwork or opponent has won,\n",
    "                # so we train NN \n",
    "                else:\n",
    "                    # Reward: 0 since game isn't over\n",
    "                    # Get q values on the state now with the new moves made\n",
    "                    newQ = model.predict(new_state.reshape(1,9), verbose=0)\n",
    "                    # Get max legal q value\n",
    "                    maxQ = self.legalQ(newQ, new_state)\n",
    "                    \n",
    "                    # vector y = qvals, differing only in the spot where the NeuralNetwork\n",
    "                    # chose to move. There, y holds discfac * maxQ. The purpose of this\n",
    "                    # is to train each move based on the best option the NeuralNetwork has in \n",
    "                    # the next state. In other words, moves get judged based on future\n",
    "                    # consequences \n",
    "                    update = (discfac * maxQ)\n",
    "                    y = np.zeros((1,9))\n",
    "                    y[:] = qvals[:] \n",
    "                    y[0][move] = update\n",
    "                    #print(\"Move: %r\" % move)\n",
    "                    #print(\"Update: %r\" % update)\n",
    "                    #print(y)\n",
    "                    # 10 epochs to make sure the behavior is enforced \n",
    "                    self.model.fit(qvals, y, nb_epoch=1, verbose=0)\n",
    "                    # set the current state to the new state\n",
    "                    state = new_state\n",
    "                    j+=1\n",
    "            \n",
    "            # Rewards: win = 10, loss = -10, tie = 0\n",
    "            # When the game is over, we train soley based on whether the NeuralNetwork won, lost, or tie\n",
    "            y = np.zeros((1,9))\n",
    "            y[:] = qvals[:]\n",
    "            y[0][move] = reward\n",
    "            model.fit(qvals, y, nb_epoch=1, verbose=0)\n",
    "            \n",
    "            # As training progresses, we make it more likely that NeuralNetwork chooses the best moves\n",
    "            if epsilon > .1: epsilon -= 1/games\n",
    "            counter += 1\n",
    "            if counter % 10000 == 0: \n",
    "                print(\"Game {0} of {1} complete.\".format(counter, games))\n",
    "                print(\"Won {0} of {1} games.\".format(games_won, counter))\n",
    "                print(\"Epsilon: \", epsilon)\n",
    "                \n",
    "            if i%1000==0:\n",
    "                j = int(i/1000)\n",
    "                model.save_weights('model_weights{0}.h5'.format(j), overwrite=True)\n",
    "                                   \n",
    "        print(\"Done. Won {0} of {1} games.\".format(games_won, games))\n",
    "\n",
    "    \n",
    "    def getReward(self, state):\n",
    "        if win(state, 1):\n",
    "            return 10\n",
    "        elif win(state, 2):\n",
    "            return -10\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # Returns a list of Q values associated with each possble move in the state given\n",
    "    def nextQs(self, state): \n",
    "        state = np.asarray(state)\n",
    "        Q, q = model.predict_proba(state.reshape(1,9), verbose=0), []\n",
    "        legal_moves = self.legal(state)\n",
    "        for i in legal_moves: q.append(Q[0][i])\n",
    "        return q\n",
    "    \n",
    "    def best_move(self, state):\n",
    "        legal_moves, q = self.legal(state), self.nextQs(state)\n",
    "        #print(\"Legal moves: {0} q: {1}\".format(legal_moves, q))\n",
    "        count = q.count(max(q))\n",
    "        if count > 1:\n",
    "            best_choices = [k for k in range(len(legal_moves)) if q[k] == max(q)]\n",
    "            move_index = random.choice(best_choices)\n",
    "        else: move_index = q.index(max(q))\n",
    "        #print \"Move_index: %d\" % move_index\n",
    "        return legal_moves[move_index]\n",
    "        \n",
    "    def play(self, state):\n",
    "        move = self.best_move(state)\n",
    "        print (\"Computer's move: {0}\".format(move+1))\n",
    "        return move\n",
    "    \n",
    "    def getType(self):\n",
    "        return \"Computer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Human:\n",
    "    def getType(self):\n",
    "        return \"Human\"\n",
    "\n",
    "    def play(self, state):\n",
    "        move = int(input(\"Your move: \"))\n",
    "        if move >0 and move < 10:\n",
    "            return move-1\n",
    "        else:\n",
    "            raise ValueError(\"Entry must be a number between 1 and 9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def play(self, player1, player2):\n",
    "        state = [0 for j in range(9)]\n",
    "        start = [' ' for i in range(9)]\n",
    "        print (board.format(*start), '\\n')\n",
    "        while True:\n",
    "            player1_move = player1.play(state)\n",
    "            start[player1_move], state[player1_move] = 'X', 1\n",
    "            print (board.format(*start), '\\n')\n",
    "            if win(state, 1): \n",
    "                player1_type = player1.getType()\n",
    "                print(\"%s wins!\" % player1_type)\n",
    "                break\n",
    "            if tie(state):\n",
    "                print (\"Tie.\")\n",
    "                break\n",
    "            player2_move = player2.play(state)\n",
    "            start[player2_move], state[player2_move] = 'O', 2\n",
    "            print (board.format(*start), '\\n')\n",
    "            if win(state, 2):\n",
    "                player2_type = player2.getType()\n",
    "                print (\"%s wins!\" % player2_type)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 10000 of 50000 complete.\n",
      "Won 7125 of 10000 games.\n",
      "Epsilon:  0.19999999999986942\n",
      "Game 20000 of 50000 complete.\n",
      "Won 14904 of 20000 games.\n",
      "Epsilon:  0.09999999999989083\n",
      "Game 30000 of 50000 complete.\n",
      "Won 22827 of 30000 games.\n",
      "Epsilon:  0.09999999999989083\n",
      "Game 40000 of 50000 complete.\n",
      "Won 30746 of 40000 games.\n",
      "Epsilon:  0.09999999999989083\n",
      "Game 50000 of 50000 complete.\n",
      "Won 38607 of 50000 games.\n",
      "Epsilon:  0.09999999999989083\n",
      "Done. Won 38607 of 50000 games.\n"
     ]
    }
   ],
   "source": [
    "CPU = NeuralNetwork()\n",
    "CPU.learnWithQ(games = 50000, discfac = 1, epsilon = 0.4, Qplayer = Q_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CPU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1f89224c3f8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuman\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#try to graph learning curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTicTacToe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'CPU' is not defined"
     ]
    }
   ],
   "source": [
    "Me = Human()   #try to graph learning curves\n",
    "T = TicTacToe()\n",
    "T.play(CPU, Me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09355083  0.10936422 -0.09579809 ..., -0.24022695  0.11623903\n",
      "  -0.04320657]\n",
      " [ 0.12701257  0.11636183  0.13358186 ...,  0.05215099 -0.00886778\n",
      "   0.04159417]\n",
      " [-0.10141278  0.24824816 -0.23343313 ..., -0.19983137 -0.09476351\n",
      "   0.24896444]\n",
      " ..., \n",
      " [-0.16166732 -0.07019818 -0.06876888 ..., -0.07381324  0.08461549\n",
      "   0.2049332 ]\n",
      " [-0.18976746 -0.0645212   0.25389743 ..., -0.13014217 -0.08680989\n",
      "  -0.019475  ]\n",
      " [-0.05022315  0.11871918  0.10238539 ..., -0.12466219 -0.14003606\n",
      "  -0.18042842]]\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('model_weights.h5')\n",
    "a = model.get_weights()\n",
    "first = a[0]\n",
    "print(a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(a[4], interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
